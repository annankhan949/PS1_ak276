---
title: "CEVE 543 Fall 2025 Problem Set 1"
subtitle: "Estimating GEV parameters and characterizing uncertainty"
author: ak276
date: "2025-09-27"

format:
  html:
    toc: true
    toc-depth: 2
    code-block-bg: "#f8f8f8"
    code-block-border-left: "#e1e4e5"
    theme: simplex
    number-sections: true
    fig-format: svg
    code-annotations: hover
  typst:
    fontsize: 11pt
    margin: 
      x: 1in
      y: 1in
    number-sections: true
    fig-format: svg
    echo: true
    code-annotations: true

execute: 
  cache: true
  freeze: auto

code-overflow: wrap
code-line-numbers: false
code-block-font-size: "0.85em"
---

```{julia}
using Pkg

Pkg.activate(".")
Pkg.instantiate()
```

```{julia}
using IJulia
using Turing
using ArviZ
using Distributions
using Random
using NCDatasets
using Optim
using Unitful
using Downloads
using Extremes
using Statistics
using ColorSchemes
using GeoMakie

using Makie.Unitful
using Makie.Dates
```

```{julia}
using CairoMakie
CairoMakie.activate!(type="svg")

using Downloads
using DataFrames
ENV["DATAFRAMES_ROWS"] = 5 

using TidierData
lab_dir = dirname(@__FILE__)
include("util.jl")


rng = MersenneTwister(345)
```

## Task 1 {.unnumbered}

### Data Extraction and Visualization

```{julia}

stations, rainfall_data = let 
    precip_fname = joinpath(lab_dir, 
    "dur01d_ams_na14v11.txt")
    url = "https://hdsc.nws.noaa.gov/pub/hdsc/data/tx/dur01d_ams_na14v11.txt" 

    if !isfile(precip_fname)
        Downloads.download(url, precip_fname)
    end
    read_noaa_data(precip_fname)
end
```

```{julia}
@chain stations begin  
	@select(stnid, noaa_id, name, state, latitude, longitude, years_of_data)  
end
```


```{julia}
stations_most = @chain stations begin
     @filter(years_of_data > 50)
     @arrange(desc(years_of_data))
     @slice_head(n=15)
end
show(stations_most[1:15, :], allrows = true)
```

```{julia}
my_stnid = 112

austin_station = @chain stations begin
    @filter(stnid == !!my_stnid)
    first
end

austin_precip = @chain rainfall_data begin
    @filter(stnid == !!my_stnid)
    @arrange(date)
end 

rainfall_conversion = Makie.UnitfulConversion(u"inch")

function plot_time_series(station_row, rainfall_df)
	fig = Figure(size = (800, 400))
	ax = Axis(fig[1, 1],
		ylabel = "Annual Maximum 24-Hour Rainfall [inch]",
		title = "$(station_row.noaa_id): $(station_row.name)",
		dim2_conversion = rainfall_conversion)

	lines!(ax, rainfall_df.date, rainfall_df.rainfall, color = :blue, linewidth = 2)
	scatter!(ax, rainfall_df.date, rainfall_df.rainfall, markersize = 10, marker = :circle, strokewidth = 2, color = :transparent)

	fig
end

plot_time_series(austin_station, austin_precip)
```

### Bayesian MLE Approach with Turing.jl
```{julia}

precip = collect(skipmissing(ustrip.(u"inch", austin_precip.rainfall)))

# Define Bayesian GEV model and priors based on data
@model function gev_model(precip)
	μ ~ Normal(2, 1)           
	log_σ ~ Normal(1.0, 0.5) #A little less than most of the data is near μ    
	ξ ~ Normal(0.1, 0.1) #Data has some extremes but those are spaced out, slightly heavy tail        

	σ = exp(log_σ)              

	precip .~ GeneralizedExtremeValue(μ, σ, ξ) 
end

austin_MLE_Turing = maximum_likelihood(gev_model(precip), NelderMead(); initial_params = [5, 0.5, 0])

μ_turing = austin_MLE_Turing.values[:μ]           
σ_turing = exp(austin_MLE_Turing.values[:log_σ])  
ξ_turing = austin_MLE_Turing.values[:ξ] 

turing_params = DataFrame(
	Parameter = ["Location (μ)", "Scale (σ)", "Shape (ξ)"],
	Value = [round(μ_turing, digits = 3), round(σ_turing, digits = 3), round(ξ_turing, digits = 3)],
)
println("GEV Parameters [Turing.jl]:")
turing_params

turing_dist = GeneralizedExtremeValue(μ_turing, σ_turing, ξ_turing)
```


### MLE Implementation with Extremes.jl
```{julia}

#Fit the GEV with MLE
austin_MLE_Extreme = gevfit(precip)

#Parameters into a data fram
μ_extremes = location(austin_MLE_Extreme)[1]  
σ_extremes = scale(austin_MLE_Extreme)[1]       
ξ_extremes = shape(austin_MLE_Extreme)[1]     

extreme_params = DataFrame(
    Parameter = ["Location (μ)", "Scale (σ)", "Shape (ξ)"],
   Value = [round(μ_extremes, digits = 3), round(σ_extremes, digits = 3), round(ξ_extremes, digits = 3)],
)
println("GEV Parameters [Extremes.jl]:")
extreme_params

extremes_dist = GeneralizedExtremeValue(μ_extremes, σ_extremes, ξ_extremes)
```

### Benchmark of Turing MLE using Extremes MLE
```{julia}
params_comparison = DataFrame(
	Method = ["Extremes MLE", "Turing MLE"],
	μ = [round(μ_extremes, digits = 3), round(μ_turing, digits = 3)],
	σ = [round(σ_extremes, digits = 3), round(σ_turing, digits = 3)],
	ξ = [round(ξ_extremes, digits = 3), round(ξ_turing, digits = 3)],
)
params_comparison

# Compare return levels
return_periods = [5, 10, 25, 50, 100]
return_levels_comparison = DataFrame(
	T_years = return_periods,
	Extremes_MLE = [round(quantile(extremes_dist, 1 - 1 / T), digits = 2) for T in return_periods],
	Turing_MLE = [round(quantile(turing_dist, 1 - 1 / T), digits = 2) for T in return_periods],
)
return_levels_comparison
#Turing likelihood is accurate as parameters match
```

```{julia}
function plot_gev_comparison(station_data, extremes_dist, turing_dist, station_info)
	fig = Figure()
	ax = Axis(fig[1, 1],
		xlabel = "Return Period (years)",
		ylabel = "Return Level (inches)",
		title = "GEV Fit Comparison\n$(station_info.noaa_id): $(station_info.name)",
		xscale = log10)  


function create_return_period_range(min_period=1.1, max_period=250, n_points=100)
    return 10 .^ range(log10(min_period), log10(max_period), length=n_points)
end

	T_smooth = create_return_period_range(1.1, 250, 100) 

	# Extremes.jl MLE curve
	levels_extremes = [quantile(extremes_dist, 1 - 1 / T) for T in T_smooth]  
	lines!(ax, T_smooth, levels_extremes, color = :blue, linewidth = 2, label = "Extremes MLE")

	# Turing.jl curve  
	levels_turing = [quantile(turing_dist, 1 - 1 / T) for T in T_smooth]  
	lines!(ax, T_smooth, levels_turing, color = :red, linewidth = 2, linestyle = :dash, label = "Turing MLE")

	# Empirical data points
	emp_levels, emp_periods = weibull_plotting_positions(station_data.rainfall) 
	scatter!(ax, emp_periods, emp_levels,
		color = :black, markersize = 8, marker = :circle,
		label = "Observed Data")

	# Standard return periods
	return_periods = [5, 10, 25, 50, 100, 250]
	ax.xticks = return_periods  

	axislegend(ax, position = :rb)  
	return fig
end

plot_gev_comparison(austin_precip, extremes_dist, turing_dist, austin_station)
```

### Bayesian Inference with MCMC
*Using the return level method as I feel I have a good understanding of rainfall in Austin, priors are based on Austin dataset and set up to follow data trends. Return levels were assumed based on looking at the scatterplot of data*

```{julia}
function load_or_sample(fname, model; overwrite=false, n_chains=4, samples_per_chain=2000, sampler=NUTS(), threading=MCMCThreads(), rng=rng)
    idata = try
        @assert !overwrite "Reading from cache disabled by overwrite=true"  
        idata = ArviZ.from_netcdf(fname)
        @info "Loaded cached prior samples from $fname"
        return idata
    catch
        chains = sample(
            model,
            sampler,
            threading,
            Int(ceil(samples_per_chain * n_chains)),  
            n_chains,
            verbose=false,
        )
        idata = ArviZ.from_mcmcchains(chains)
        ArviZ.to_netcdf(idata, fname)
        @info "Sampled and cached prior samples to $fname"
        return idata
    end
end
```

*Return levels estimated based on data scatterplot*
```{julia}
return_level_priors = [
    ReturnLevelPrior(2, 2.0, 1.0),
    ReturnLevelPrior(10, 4.0, 2.5),
    ReturnLevelPrior(50, 10.0, 6),
    ReturnLevelPrior(100, 12.0, 8),
]
```

```{julia}
@model function gev_model_quantile_priors(y; return_level_priors=[])
	μ ~ Normal(2, 1)           
	log_σ ~ Normal(1.0, 0.5)  
	ξ ~ Normal(0.1, 0.1)
    σ = exp(log_σ)
    dist = GeneralizedExtremeValue(μ, σ, ξ)

	for prior in return_level_priors
		rl = quantile(dist, prior.quantile)
		if rl > 0.1 
		Turing.@addlogprob!(loglikelihood(prior.distribution, rl))  # <2>
        else
            Turing.@addlogprob!(-Inf)  # <3>
        end
    end

	if length(y) > 0 
		y .~ dist
	end
end

prior_idata = let
    fname = joinpath(lab_dir, "prior.nc")
    model = gev_model_quantile_priors([]; return_level_priors=return_level_priors)
    overwrite = true
    load_or_sample(fname, model; overwrite=false)
end
prior_GEVs = vec(GeneralizedExtremeValue.(prior_idata.posterior.μ, exp.(prior_idata.posterior.log_σ), prior_idata.posterior.ξ))
```

```{julia}
fig_priorDist = let
    rts = logrange(1.1, 250, 500)
    xticks = [1, 2, 5, 10, 25, 50, 100, 250]
    fig = Figure(size=(1200, 600))
    ax = Axis(fig[1, 1], xlabel="Return Period (years)", ylabel="Return Level (inches)", title="Prior Predictive Distribution", xscale=log10, xticks=xticks)
    posterior_bands!(ax, prior_GEVs, rts; color=(:blue, 0.2), ci=0.90)
    posterior_mean_curve!(ax, prior_GEVs, rts; color=:blue, linewidth=3)
    fig
end
```

### Posterior Distribution vs MLE Results
```{julia}
y_obs = collect(skipmissing(ustrip.(u"inch", austin_precip.rainfall)))

bayes = gev_model_quantile_priors(y_obs; return_level_priors=return_level_priors)
```

```{julia}
mle_estimate = maximum_likelihood(bayes, NelderMead(); maxiters=1_000)

mle_dist = GeneralizedExtremeValue(mle_estimate.values[1], exp(mle_estimate.values[2]), mle_estimate.values[3])
```

```{julia}
posterior_idata = let
    fname = joinpath(lab_dir, "posterior_data.nc")
    overwrite = true
    load_or_sample(fname, bayes; overwrite=overwrite)
end
posterior_GEVs = vec(GeneralizedExtremeValue.(posterior_idata.posterior.μ, exp.(posterior_idata.posterior.log_σ), posterior_idata.posterior.ξ))

post_summary = ArviZ.summarize(posterior_idata)

post_summary
```

```{julia}
function plot_trace_diagnostics(idata, title_prefix="")  #
    fig = Figure(size=(1200, 400))

    
    μ_arr = Array(idata.posterior.μ)  
    log_σ_arr = Array(idata.posterior.log_σ)  
    ξ_arr = Array(idata.posterior.ξ)  
    ndraws, nchains = size(μ_arr)  

    ax1 = Axis(fig[1, 1], xlabel="Iteration", ylabel="μ",
        title="$title_prefix Location Parameter Trace")
    for c in 1:nchains  
        lines!(ax1, 1:ndraws, μ_arr[:, c], label="Chain $(c)")  
    end
    axislegend(ax1; position=:lb)

    ax2 = Axis(fig[1, 2], xlabel="Iteration", ylabel="log(σ)",
        title="$title_prefix Log-Scale Parameter Trace")
    for c in 1:nchains
        lines!(ax2, 1:ndraws, log_σ_arr[:, c])
    end

    ax3 = Axis(fig[1, 3], xlabel="Iteration", ylabel="ξ",
        title="$title_prefix Shape Parameter Trace")
    for c in 1:nchains
        lines!(ax3, 1:ndraws, ξ_arr[:, c])
    end

    return fig
end
fig_trace_primary = plot_trace_diagnostics(posterior_idata, "Primary Station:")
```

*Good distribution between bulk and tail portions and r_hat of 1, model converges*

```{julia}
let
    fig = Figure(size=(900, 500))
    rts = logrange(1.1, 250, 500)
    ax1 = Axis(fig[1, 1], xlabel="Return Period (years)", ylabel="Return Level (inches)",
        title="Return Level Uncertainty: Prior vs Posterior", xscale=log10, xticks=[1, 2, 5, 10, 25, 50, 100, 250])
    posterior_bands!(ax1, prior_GEVs, rts; color=(:blue, 0.2), ci=0.90, label="Prior 90% CI")
    posterior_bands!(ax1, posterior_GEVs, rts; color=(:orange, 0.4), ci=0.90, label="Posterior 90% CI")
    posterior_mean_curve!(ax1, posterior_GEVs, rts; color=:blue, linewidth=3, label="Posterior Mean")

    mean_return_levels = [quantile(mle_dist, 1 - 1 / T) for T in rts]
    lines!(ax1, rts, mean_return_levels, color=:red, linewidth=3, label="MLE")

    axislegend(ax1; position=:lt)
    fig
end
```

### 50 and 100-year Return Estimates

```{julia}
rl_50 = [quantile(dist, 1 - 1/50) for dist in posterior_GEVs]
rl_100 = [quantile(dist, 1 - 1/100) for dist in posterior_GEVs]

# 95% uncertainty bounds
rl_50_ci = quantile(rl_50, [0.025, 0.975])  
rl_100_ci = quantile(rl_100, [0.025, 0.975])

fig_return_hists = let
    fig = Figure(size=(1000, 400))

    # Histogram for 50-year return level
    ax1 = Axis(fig[1, 1], xlabel="50-Year Return Level (inches)", ylabel="Frequency",
               title="Posterior: 50-Year Return Level")
    hist!(ax1, rl_50, bins=30, color=(:blue, 0.6), strokecolor=:black)
    vlines!(ax1, [rl_50_ci[1], rl_50_ci[2]], color=:black, linestyle=:dash)
    # Histogram for 100-year return level
    ax2 = Axis(fig[1, 2], xlabel="100-Year Return Level (inches)", ylabel="Frequency",
               title="Posterior: 100-Year Return Level")
    hist!(ax2, rl_100, bins=30, color=(:green, 0.6), strokecolor=:black)
    vlines!(ax2, [rl_100_ci[1], rl_50_ci[2]], color=:black, linestyle=:dash)
    fig
end
```

## Task 2 {.unnumbered}

### Multi-Station GEV 
```{julia}
nearest_stations = find_nearest_stations(austin_station, stations, 5)

nearest_stnids = [789, 10, 787, 752]
```

*going to use the same priors as I did for the Austin station, assuming the stations are similar in geographical properties to Austin considering they are close by*

```{julia}
function Turing_MLE_Params(stnid, stations, rainfall_data)
    # Dictionary to hold a DataFrame of GEV parameters for each station
    results = Dict()

    for stn_id in stnid
         #1. Filter station row
        station_row = filter(row -> row.stnid == stn_id, stations)
        station_row = first(station_row, 1)  # assuming only one row per stnid

        #2. Filter rainfall data for this station and sort by date
        station_data = filter(row -> row.stnid == stn_id, rainfall_data)
        sort!(station_data, :date)

        # 3. Preprocess rainfall values
        precip = collect(skipmissing(ustrip.(u"inch", station_data.rainfall)))

        # 4. Define and fit the model
        @model function gev_model(precip)
            μ ~ Normal(2, 1)
            log_σ ~ Normal(1.0, 0.5)
            ξ ~ Normal(0.1, 0.1)
            σ = exp(log_σ)
            precip .~ GeneralizedExtremeValue(μ, σ, ξ)
        end

        mle_result = maximum_likelihood(
            gev_model(precip), 
            NelderMead(); 
            initial_params = [2.0, 0.5, 0.1]
        )

        # 5. Extract and store parameters
        μ_t = mle_result.values[:μ]
        σ_t = exp(mle_result.values[:log_σ])
        ξ_t = mle_result.values[:ξ]

        param_df = DataFrame(
            Parameter = ["Location (μ)", "Scale (σ)", "Shape (ξ)"],
            Value = [μ_t, σ_t, ξ_t],
            Rounded = round.([μ_t, σ_t, ξ_t]; digits=3),
        )

        results[stn_id] = param_df
    end

    return results
end
```

```{julia}
Combined_results = Turing_MLE_Params(nearest_stnids, stations, rainfall_data)
for (stn_id, df) in Combined_results
    println("Results for station ID: $stn_id")
    display(df)
    println("\n" * "#"^40 * "\n")  #looked up a seperator to make Turing GEV results easier to view
end
```

*Results from each station are similar but not the exact same as eachother or the Austin Station*

### Multi-Station 50-year Return Level Estimate Comparison

```{julia}
function return_level_gev(μ, σ, ξ, T)
    return_lvl = [round(quantile(extremes_dist, 1 - 1 / T), digits = 2)]
end

```

```{julia}
return_period = 50 
return_level_summary = DataFrame(Station_ID = Int[], Return_Level = Float64[])

for (stn_id, df) in Combined_results
    μ = df[df.Parameter .== "Location (μ)", :Value][1]
    σ = df[df.Parameter .== "Scale (σ)", :Value][1]
    ξ = df[df.Parameter .== "Shape (ξ)", :Value][1]

    # Compute return level
    rl = return_level_gev(μ, σ, ξ, return_period)

    # Store the result
    push!(return_level_summary, (
    Station_ID = stn_id,
    Return_Level = round(return_level_gev(μ, σ, ξ, return_period)[1]; digits=3)
))
end

# Display the summary table
println("Return Level Summary for $return_period-Year Period:")
display(return_level_summary)
```

### Time Series of All 5 Stations

```{julia}
all_stnid = [112, 10, 787, 752, 468]
```

```{julia}
function plot_nearby_comparison(stations_df, rainfall_data, stnid)
    fig = Figure(size = (800, 400))
    ax = Axis(fig[1, 1],
        ylabel = "Annual Maximum Rainfall [inch]",
        title = "Comparison of Austin and 4 Nearby Stations",
        dim2_conversion = rainfall_conversion)

    colors = [get(colorschemes[:viridis], i / length(stnid)) for i in 0:length(stnid)-1]

    for (i, stnid) in enumerate(stnid)

        station_data = filter(row -> row.stnid == stnid, rainfall_data)
        sort!(station_data, :date)

        station_row = filter(row -> row.stnid == stnid, stations_df)
        station_row = first(station_row)

        lines!(ax, station_data.date, station_data.rainfall,
               color = colors[i], linewidth = 2,
               label = "$(station_row.noaa_id): $(station_row.name)")
        scatter!(ax, station_data.date, station_data.rainfall,
                 color = colors[i], markersize = 8)
    end

    axislegend(ax, position = :lt, fontsize = 4, patchsize = (4, 4))
    return fig
end

```

```{julia}
plot_nearby_comparison(stations, rainfall_data, all_stnid)
```

## Task 3 {.unnumbered}
```{julia}
using LinearAlgebra: I
using LaTeXStrings
```

### Mann Kendall on Austin Station

```{julia}
function mann_kendall_test(x::AbstractVector)
    n = length(x)
    S = sum(sign(x[j] - x[i]) for i in 1:(n-1) for j in (i+1):n)

    var_S = n * (n - 1) * (2n + 5) / 18

    Z = if S > 0
        (S - 1) / sqrt(var_S)
    elseif S < 0
        (S + 1) / sqrt(var_S)
    else
        0.0
    end

    p_value = 2 * (1 - cdf(Normal(0, 1), abs(Z)))

    return S, p_value
end
```

```{julia}
mk_S, mk_p = mann_kendall_test(precip) #Using austin precip that was defined earlier
show(mk_S)
show(mk_p)
```

*Our Mann-Kendall Test Statistic is -693 which is quite a large number and would indicate a decreasing trend, but our p-value being so large (0.18) makes the prediction statistically insignificant (significant pvalue < 0.05)*

```{julia}
# Get 4 nearest stations that are NOT Austin
nearest_stations = @chain stations begin
    filter(row -> row.stnid != austin_station.stnid, _)
    find_nearest_stations(austin_station, _, 4)
end

# Get Austin station row
austin_row = filter(row -> row.stnid == austin_station.stnid, stations)
austin_row = first(austin_row, 1)

# Combine Austin with 4 nearby stations 
MK_stations = vcat(austin_row, nearest_stations)
```

### MK on Multiple Stations
```{julia}
let
    stnids = MK_stations.stnid
    raw_results = map(stnids) do stnid
        df = @chain rainfall_data begin
            @filter(stnid == !!stnid)
            @filter(!ismissing(rainfall))
        end
        prcp = ustrip.(u"inch", df.rainfall)
        mk_S, mk_p = mann_kendall_test(prcp)
        return mk_S, mk_p
    end
    MK_stations[!, :mk_S] = getindex.(raw_results, 1)
    MK_stations[!, :mk_pvalue] = getindex.(raw_results, 2)
end
show(MK_stations)
```

*Interestingly enough the S statistic for all the stations are far different from the Austin station despite being geographically close. This could be due to amount of data and the only statistically significant test was from station 787 Astin Bergstrom AP*

### Fit at least two nonstationary GEV

*The covariate I chose was annual mean temperature as a hotter atmosphere can lead to more accumilation of moisture. Data was recieved from https://www.weather.gov/wrh/climate?wfo=ewx and was passed through excel before it was exported as a csv*

```{julia}
temp_data = let
    temp_fname = joinpath(lab_dir, "Austin_annualTemp.csv")
    TidierFiles.read_csv(temp_fname) |> DataFrame
end
```

*Location Varying, here only the location parameter varies with the mean annual temperature*
```{julia}
@model function nonstationary_gev_model1(y, x) #y is rainfall, x is temp 
    # Model 1: μ(x) = α_μ + β_μ*x where x = 69.9
    α_μ ~ Normal(2.0, 1.0)    # baseline location parameter
    β_μ ~ Normal(0.0, 2.0)    # location trend parameter (inches per log(ppm))
    log_σ ~ Normal(1.0, 0.5)  # log-scale parameter
    ξ ~ Normal(0.1, 0.1)      # shape parameter

    σ = exp(log_σ)

    # Location parameter varies with temp
    for i in eachindex(y)
        x_centered = x[i] - 69.9  # midpoint at 2005 @ 69.9
        μ_x = α_μ + β_μ * x_centered
        dist = GeneralizedExtremeValue(μ_x, σ, ξ)
        y[i] ~ dist
    end
end
```

*Location and scale varying model, here both Locationa and Scale parameters vary with the mean annual temperature*
```{julia}
@model function nonstationary_gev_model2(y, x)
    # Model 2: μ(x) = α_μ + β_μ*x, σ(x) = α_σ + β_σ*x
    α_μ ~ Normal(2.0, 1.0)      # baseline location parameter
    β_μ ~ Normal(0.0, 2.0)      # location trend parameter
    α_σ ~ LogNormal(1.0, 0.50)   # baseline scale parameter
    β_σ ~ Normal(0.0, 0.2)      # scale trend parameter (small prior)
    ξ ~ Normal(0.1, 0.1)        # shape parameter

    for i in eachindex(y)
        x_centered = x[i] - 69.9
        μ_x = α_μ + β_μ * x_centered
        σ_x = α_σ + β_σ * x_centered

        # Ensure positive scale parameter
        if σ_x > 0.1
            dist = GeneralizedExtremeValue(μ_x, σ_x, ξ)
            y[i] ~ dist
        else
            Turing.@addlogprob!(-Inf)
        end
    end
end
```

*Data cleaning*
```{julia}
my_rainfall = @chain austin_precip begin
    @arrange(date)
    @full_join(temp_data, "year")  
    @arrange(year)
end

my_rainfall_nomissing = @chain my_rainfall begin
    @filter(!ismissing(rainfall) && !ismissing(temp))
end
```

```{julia}
y_obs = ustrip.(u"inch", my_rainfall_nomissing.rainfall)
x_obs = my_rainfall_nomissing.temp

# Fit the two models
models = [
    ("Location trend", nonstationary_gev_model1(y_obs, x_obs)),
    ("Location + Scale trends", nonstationary_gev_model2(y_obs, x_obs)),
]

# Sample from posteriors and check diagnostics
posterior_results = []
for (name, model) in models
    fname = joinpath(lab_dir, "nonstat_$(replace(name, " " => "_")).nc")
    overwrite = false
    idata = load_or_sample(fname, model; overwrite=overwrite, samples_per_chain=1000)
    push!(posterior_results, (name=name, idata=idata))

    # Check diagnostics immediately after fitting
    println("=== Diagnostics for $name ===")
    display(ArviZ.summarize(idata))
end
```

*R Hat of 1 for all and good distribution amougnst bulk and tail indicate models converge*

### Return Level Plots

*First we need to extract GEV distributions*
```{julia}
# Model 1: Location trend only
function extract_model1_gevs(idata, x)
    x_centered = x - 69.9
    α_μ = Array(idata.posterior[:α_μ])
    β_μ = Array(idata.posterior[:β_μ])
    σ = exp.(Array(idata.posterior[:log_σ]))
    ξ = Array(idata.posterior[:ξ])
    μ_x = α_μ .+ β_μ .* x_centered
    vec(GeneralizedExtremeValue.(μ_x, σ, ξ))
end

# Model 2: Location + Scale trends
function extract_model2_gevs(idata, x)
    x_centered = x - 69.9
    α_μ = Array(idata.posterior[:α_μ])
    β_μ = Array(idata.posterior[:β_μ])
    α_σ = Array(idata.posterior[:α_σ])
    β_σ = Array(idata.posterior[:β_σ])
    ξ = Array(idata.posterior[:ξ])
    μ_x = α_μ .+ β_μ .* x_centered
    σ_x = α_σ .+ β_σ .* x_centered
    # Filter out negative scale parameters
    valid = σ_x .> 0.1
    vec(GeneralizedExtremeValue.(μ_x[valid], σ_x[valid], ξ[valid]))
end
```

*Now lets extract data from 1898 (first occurance with both rainfall and temp data) and 2017 (last occurance with both rainfall and temp data)*

```{julia}
model1_name, model1_idata = posterior_results[1].name, posterior_results[1].idata
model2_name, model2_idata = posterior_results[2].name, posterior_results[2].idata

# Approximate temp levels (x = 69.9)
x_1950 = temp_data.temp[temp_data.year.==1898][1]   
x_2025 = temp_data.temp[temp_data.year.==2017][1]  

# Extract GEV distributions for both time periods
gevs_1950 = [
    extract_model1_gevs(model1_idata, x_1950),
    extract_model2_gevs(model2_idata, x_1950),
]

gevs_2025 = [
    extract_model1_gevs(model1_idata, x_2025),
    extract_model2_gevs(model2_idata, x_2025),
]
```

*Now we generate the return level plot*

```{julia}
fig_comprehensive = let
    fig = Figure(size=(1000, 700))

    rts = logrange(1.1, 250, 100)
    xticks = [2, 5, 10, 25, 50, 100, 250]

    ax1 = Axis(fig[1, 1], xlabel="Return Period (years)", ylabel="Return Level (inches)",
        title="Location Trend Model", xscale=log10, xticks=xticks)
    ax2 = Axis(fig[1, 2], xlabel="Return Period (years)", ylabel="Return Level (inches)",
        title="Location + Scale Trends Model", xscale=log10, xticks=xticks)

    # Make columns equal width
    colsize!(fig.layout, 1, Relative(0.5))
    colsize!(fig.layout, 2, Relative(0.5))

    top_axes = [ax1, ax2]

    for (i, (ax, gevs_50, gevs_25)) in enumerate(zip(top_axes, gevs_1950, gevs_2025))
        posterior_bands!(ax, gevs_50, rts; ci=0.90, color=(:blue, 0.3))
        posterior_mean_curve!(ax, gevs_50, rts; color=:blue, linewidth=2, label="1950")
        posterior_bands!(ax, gevs_25, rts; ci=0.90, color=(:red, 0.3))
        posterior_mean_curve!(ax, gevs_25, rts; color=:red, linewidth=2, label="2025")
        if i == 1
            axislegend(ax, position=:rb)
        end
    end

    
    ax3 = Axis(fig[2, 1:2], xlabel="Return Period (years)", ylabel="Return Level (inches)",
        title="Model Comparison for 2017",
        xscale=log10, xticks=xticks)

    colors = [:blue, :red]
    labels = ["Location Trend", "Location + Scale"]

    for (i, (gevs, color, label)) in enumerate(zip(gevs_2025, colors, labels))
        posterior_bands!(ax3, gevs, rts; ci=0.68, color=(color, 0.3))
        posterior_mean_curve!(ax3, gevs, rts; color=color, linewidth=2, label=label)
    end

    axislegend(ax3, position=:lt)
    linkyaxes!(top_axes...)

    fig
end
```

### Regional Parameter Estimation

*I will vary both location and scale as when we ran this for a single station that provided less uncertainty. Originally I thought to only vary the location parameter as the scale parameter is less variable over regions but given the uncertainty difference in the single station runs I want to do both.*
```{julia}
#Like the lab I am going to pick stations based on lon/lat but I am going to select the first 10 for a bigger pool of data
analysis_stations = let
    lon = austin_station.longitude
    lat = austin_station.latitude
    @chain stations begin
        @filter(years_of_data >= 40)
        @mutate(distance = calc_distance(!!lat, !!lon, latitude, longitude))
        @arrange(distance)
        first(10)
    end
end
analysis_stnids = analysis_stations.stnid
#Need to turn stnid's into a matrix with all data and seperate after 
years_vec, rainfall_matrix = let
    rainfall_matrix_data = @chain rainfall_data begin
        @filter(in(stnid, !!analysis_stnids))
        @mutate(rainfall_inch = ifelse(ismissing(rainfall), missing, ustrip(u"inch", rainfall)))
        @select(year, stnid, rainfall_inch)
        @pivot_wider(names_from = stnid, values_from = rainfall_inch)
        @arrange(year)
    end
    years = rainfall_matrix_data.year
    matrix = Matrix(rainfall_matrix_data[:, 2:end])
    years, matrix
end
#Now seperating into x and y vectors where y will store rainfall and x will store temp, had to do some funky stuff here since the temp and rainfall sets had mismatching rows/columns

# Extract rainfall data for selected stations and pivot wider
rainfall_wide = @chain rainfall_data begin
    @filter(in(stnid, !!analysis_stnids))
    @mutate(rainfall_inch = ifelse(ismissing(rainfall), missing, ustrip(u"inch", rainfall)))
    @select(year, stnid, rainfall_inch)
    @pivot_wider(names_from = stnid, values_from = rainfall_inch)
    @arrange(year)
end

# Extract temperature data for all years available
temp_df = @chain temp_data begin
    @arrange(year)
    @select(year, temp)
end

# Find common years
common_years = intersect(rainfall_wide.year, temp_df.year)

# Filter both datasets to these common years
rainfall_wide = @chain rainfall_wide begin
    @filter(in(year, !!common_years))  
    @arrange(year)
end

temp_df = @chain temp_df begin
    @filter(in(year, !!common_years))
    @arrange(year)
end

# Convert rainfall to matrix and temp to vector
y_matrix = Matrix(rainfall_wide[:, Not(:year)])
x_vector = temp_df.temp

```

*Now I am going to implement the Regional Model, the model from lab seems to have varying scale and location as well.*

```{julia}
@model function regional_nonstationary_gev(y_matrix, x_vector)

    n_years, n_stations = size(y_matrix)

    # Regional parameters (shared across all stations)
    β_region ~ Normal(0.0, 2.0)          # Regional trend (inches per log(ppm))
    ξ_region ~ Normal(0.0, 0.2)          # Regional shape parameter

    # Station-specific parameters (independent for each station)
    α_μ_stations ~ MvNormal(fill(3.0, n_stations), I * 2.0)  # Baseline location for each station
    log_σ_stations ~ MvNormal(zeros(n_stations), I * 0.5)    # Scale parameter for each station

    σ_stations = exp.(log_σ_stations)

    # Data likelihood - loop over matrix, skip missing values
    for i in 1:n_years
        x_centered = x_vector[i] - 69.9
        for j in 1:n_stations
            if !ismissing(y_matrix[i, j])
                μ_ij = α_μ_stations[j] + β_region * x_centered
                dist = GeneralizedExtremeValue(μ_ij, σ_stations[j], ξ_region)
                y_matrix[i, j] ~ dist
            end
        end
    end
end

# Fit regional model with diagnostics
regional_idata = let
    regional_fname = joinpath(lab_dir, "regional_nonstat.nc")
    regional_model = regional_nonstationary_gev(y_matrix, x_vector)
    overwrite = true
    idata2 = load_or_sample(regional_fname, regional_model; overwrite=overwrite, samples_per_chain=1500)

    # Check diagnostics immediately after fitting
    println("=== Regional Model Diagnostics ===")
    display(ArviZ.summarize(idata2))

    idata2
end
```

### Posterior Uncertainty 50-Year Return Levels

*First we need to extract the GEVs for both single and multi station runs*
```{julia}
my_station_idx = findfirst(x -> x == my_stnid, analysis_stnids)
regional_my_station = let
    # Extract regional parameters (shared)
    β_samples = vec(Array(regional_idata.posterior[:β_region]))
    ξ_samples = vec(Array(regional_idata.posterior[:ξ_region]))

    # Extract station-specific parameters for our station
    α_μ_samples = vec(Array(regional_idata.posterior[:α_μ_stations])[:, :, my_station_idx])
    σ_samples = exp.(vec(Array(regional_idata.posterior[:log_σ_stations])[:, :, my_station_idx]))

    (α_μ=α_μ_samples, β_μ=β_samples, σ=σ_samples, ξ=ξ_samples)
end

# Extract single-station model results (Model 1: Location trend only)
single_station = let
    model1_idata = posterior_results[1].idata
    α_μ_samples = vec(Array(model1_idata.posterior[:α_μ]))
    β_μ_samples = vec(Array(model1_idata.posterior[:β_μ]))
    σ_samples = exp.(vec(Array(model1_idata.posterior[:log_σ])))
    ξ_samples = vec(Array(model1_idata.posterior[:ξ]))

    (α_μ=α_μ_samples, β_μ=β_μ_samples, σ=σ_samples, ξ=ξ_samples)
end

# Prepare data for comparison plots
rts = logrange(1.1, 250, 100)
xticks = [2, 5, 10, 25, 50, 100, 250]
x_2025 = temp_data.temp[temp_data.year.==2024][1]
x_centered = x_2025 - log(380)

# Create GEV distributions for 2025
μ_single_2025 = single_station.α_μ .+ single_station.β_μ .* x_centered
gevs_single = GeneralizedExtremeValue.(μ_single_2025, single_station.σ, single_station.ξ)

μ_regional_2025 = regional_my_station.α_μ .+ regional_my_station.β_μ .* x_centered
gevs_regional = GeneralizedExtremeValue.(μ_regional_2025, regional_my_station.σ, regional_my_station.ξ)
```

*Now we can plot the GEVs for comparison of their 50 year Return Levels:*
```{julia}
return_level_dist_fig = let
    fig = Figure(size=(800, 400))

    ax = Axis(fig[1, 1], xlabel="50-year Return Level (inches)", ylabel="Density",
        title="50-year Return Level Uncertainty")

    # Calculate 100-year return levels
    rl50_single = [quantile(gev, 0.99) for gev in gevs_single]
    rl50_regional = [quantile(gev, 0.99) for gev in gevs_regional]

    hist!(ax, rl50_single, bins=30, color=(:blue, 0.5), label="Single-Station", normalization=:pdf)
    hist!(ax, rl50_regional, bins=30, color=(:red, 0.5), label="Regional", normalization=:pdf)

    axislegend(ax, position=:rt)

    fig
end

```

*Regional seems to have some sort of issue (maybe stemming from all the data manipulation I had to do to make temp and rainfall have same length???) that leads to a high density outlier*